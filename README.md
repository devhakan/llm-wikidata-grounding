# LLM Wikidata Grounding

A fact-checking system that grounds claims against Wikidata's structured knowledge base using vector search, cross-encoder reranking, and NLI classification.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ What This Project Does

LLMs are powerful but prone to **hallucinations** â€” generating plausible-sounding but incorrect information. This project verifies claims against Wikidata using a multi-stage pipeline:

```
Claim â†’ Vector Search â†’ Statement Retrieval â†’ Reranking â†’ NLI â†’ Verdict
```

### Example Results

| Claim | Verdict | Confidence |
|-------|---------|------------|
| "Ibn al-Haytham was born in Basra" | âœ“ SUPPORTED | 76% |
| "Aziz Sancar won the Nobel Prize in Chemistry in 2015" | âœ“ SUPPORTED | 58% |
| "Ã–zlem TÃ¼reci is the co-founder of BioNTech" | âœ“ SUPPORTED | 73% |
| "Al-Khwarizmi lived in the 9th century" | âœ“ SUPPORTED | 73% |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLAIM: "Aziz Sancar won Nobel Prize in Chemistry in 2015"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. VECTOR SEARCH                                                    â”‚
â”‚     Query: wd-vectordb.wmcloud.org                                   â”‚
â”‚     Result: Q15118973 (Aziz Sancar), Q44585 (Nobel Chemistry)...     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. STATEMENT RETRIEVAL                                              â”‚
â”‚     API: wd-textify.toolforge.org                                    â”‚
â”‚     Result: 1225 statements about matched entities                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. CROSS-ENCODER RERANKING                                          â”‚
â”‚     Model: cross-encoder/ms-marco-MiniLM-L-6-v2                      â”‚
â”‚     Result: Top 10 relevant statements (score: 1.00)                 â”‚
â”‚       â€¢ "Aziz Sancar | award received | Nobel Prize in Chemistry"    â”‚
â”‚       â€¢ "Aziz Sancar | description | Nobel Prize Chemistry 2015"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. NLI CLASSIFICATION                                               â”‚
â”‚     Model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli                   â”‚
â”‚     Premise: "Aziz Sancar received Nobel Prize Chemistry 2015"       â”‚
â”‚     Hypothesis: "Aziz Sancar won Nobel Prize Chemistry in 2015"      â”‚
â”‚     Result: ENTAILMENT (58% confidence)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERDICT: âœ“ SUPPORTED                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Installation

### Prerequisites

- Python 3.10+
- ~4GB disk space (for ML models)
- ~8GB RAM recommended

### Quick Start

```bash
# Clone
git clone https://github.com/devhakan/llm-wikidata-grounding.git
cd llm-wikidata-grounding

# Virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install
pip install -r requirements.txt

# Verify
python verify_setup.py
```

> **Note**: First run downloads ~650MB of ML models. Subsequent runs use cached models.

---

## ğŸš€ Usage

### Command Line

```bash
# Single claim
python -m src.pipeline "Aziz Sancar won Nobel Prize in Chemistry"

# Verbose mode
python -m src.pipeline -v "Ibn al-Haytham was born in Basra"

# Interactive mode
python -m src.pipeline
```

### Python API

```python
from src import FactChecker, verify

# Quick check
result = verify("Ã–zlem TÃ¼reci is the co-founder of BioNTech")
print(result.verdict)      # VerificationResult.SUPPORTED
print(result.confidence)   # 0.73
print(result.evidence)     # ["Ã–zlem TÃ¼reci | affiliation | BioNTech", ...]

# With options
checker = FactChecker(verbose=True)
result = checker.check("Al-Khwarizmi lived in the 9th century")
```

### Example Script

```bash
python examples/basic_example.py
```

---

## ğŸ”¬ Components

### 1. Vector Search

Semantic search using Wikidata's experimental vector database:

```python
from src.wikidata_api import vector_search

# Find entities semantically similar to the query
results = vector_search("Turkish scientist who won Nobel Prize")
# â†’ [{"id": "Q15118973", "score": 0.86}, ...]  # Aziz Sancar
```

**API**: `wd-vectordb.wmcloud.org` (no API key required)

### 2. Cross-Encoder Reranking

Filter thousands of statements to the most relevant ones:

```python
from src.reranker import Reranker

reranker = Reranker()
ranked = reranker.rerank(
    claim="Sancar won Nobel Prize",
    statements=all_statements,  # 1000+ statements
    top_k=10                    # Keep top 10
)
# â†’ [RankedStatement(text="Sancar | award | Nobel Prize", score=1.0), ...]
```

**Model**: `cross-encoder/ms-marco-MiniLM-L-6-v2` (~90MB)

### 3. NLI Classification

Determine if evidence supports or contradicts the claim:

```python
from src.nli_classifier import NLIClassifier

classifier = NLIClassifier()
result = classifier.classify(
    premise="Ibn al-Haytham's place of birth is Basra",
    hypothesis="Ibn al-Haytham was born in Basra"
)
# â†’ ClassificationResult(verdict=ENTAILMENT, confidence=0.76)
```

**Model**: `MoritzLaurer/mDeBERTa-v3-base-mnli-xnli` (~558MB, multilingual)

---

## ğŸ“Š Verdict Types

| Verdict | Meaning | NLI Result |
|---------|---------|------------|
| âœ“ **SUPPORTED** | Evidence confirms the claim | ENTAILMENT |
| âœ— **REFUTED** | Evidence contradicts the claim | CONTRADICTION |
| ? **NOT_ENOUGH_INFO** | Can't verify or refute | NEUTRAL |

---

## ğŸŒ Multilingual Support

The NLI model (`mDeBERTa`) supports 100+ languages:

```python
# Turkish
verify("Aziz Sancar 2015'te Nobel Kimya Ã–dÃ¼lÃ¼ kazandÄ±")

# German
verify("Einstein entdeckte die RelativitÃ¤tstheorie")

# Arabic
verify("Ø§Ø¨Ù† Ø§Ù„Ù‡ÙŠØ«Ù… ÙˆÙ„Ø¯ ÙÙŠ Ø§Ù„Ø¨ØµØ±Ø©")
```

---

## âš™ï¸ Configuration

### Custom Models

```python
checker = FactChecker(
    reranker_model="BAAI/bge-reranker-base",           # Better quality
    nli_model="microsoft/deberta-v3-large-mnli",       # Higher accuracy
    verbose=True
)
```

### Environment Variables

```bash
# Force CPU (if GPU issues)
CUDA_VISIBLE_DEVICES=-1

# HuggingFace token (optional, for faster downloads)
HF_TOKEN=your_token
```

---

## ğŸ“ Project Structure

```
llm-wikidata-grounding/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ wikidata_api.py    # Vector search + Wikidata APIs
â”‚   â”œâ”€â”€ reranker.py        # Cross-Encoder reranking
â”‚   â”œâ”€â”€ nli_classifier.py  # NLI classification
â”‚   â”œâ”€â”€ pipeline.py        # Main fact-checking pipeline
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ basic_example.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ HOW_IT_WORKS.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ verify_setup.py
â””â”€â”€ README.md
```

---

## ğŸ¤ Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

```bash
# Development setup
git clone https://github.com/devhakan/llm-wikidata-grounding.git
cd llm-wikidata-grounding
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python verify_setup.py
```

---

## ğŸ“š Resources

- [Wikidata Vector Database](https://www.wikidata.org/wiki/Wikidata:Vector_Database)
- [Wikidata Query Service](https://query.wikidata.org/)
- [Cross-Encoders (SBERT)](https://www.sbert.net/examples/applications/cross-encoder/README.html)
- [NLI with Transformers](https://huggingface.co/tasks/text-classification)

---

## ğŸ™ Acknowledgments

- **Philippe Saade** (Wikimedia Deutschland) - Wikidata Vector Database & workshop
- **Jonathan Fraine** (Wikimedia) - [Wikidata in the AI Web](https://commons.wikimedia.org/wiki/File:Wikidata_in_the_AI_Web_-_Lightning_Talks_Futures_Lab.pdf)
- **Wikidata community** - Maintaining the knowledge base
- **HuggingFace** - Pre-trained models

---

## ï¿½ Author

Created by **[User:HakanIST](https://www.wikidata.org/wiki/User:HakanIST)** - Wikimedia volunteer & Wikidata contributor.

- ğŸŒ [Wikidata User Page](https://www.wikidata.org/wiki/User:HakanIST)
- ğŸ’» [GitHub](https://github.com/devhakan)

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE)

---

## âš ï¸ Limitations

- **First run**: Downloads ~650MB of models
- **Speed**: 5-15 seconds per claim (includes API calls)
- **Coverage**: Not all facts are in Wikidata
- **Confidence**: Lower confidence â‰  wrong, may just need more evidence
